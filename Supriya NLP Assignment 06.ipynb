{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What are Vanilla autoencoders\n",
    "\n",
    "\"\"\"Vanilla autoencoders, also known as basic autoencoders or traditional autoencoders, are a type of \n",
    "neural network architecture used in unsupervised learning tasks, particularly in the domain of \n",
    "dimensionality reduction and data compression. The primary goal of an autoencoder is to learn a\n",
    "compact representation of the input data by training the network to reconstruct the input data \n",
    "as accurately as possible.\n",
    "\n",
    "The vanilla autoencoder consists of two main components:\n",
    "\n",
    "1. **Encoder**: This part of the network compresses the input data into a lower-dimensional \n",
    "representation, often called the \"latent space\" or \"encoding.\" The encoder typically consists\n",
    "of one or more layers of neurons that map the input data to a compressed representation.\n",
    "\n",
    "2. **Decoder**: The decoder takes the compressed representation produced by the encoder and\n",
    "attempts to reconstruct the original input data from it. It consists of one or more layers \n",
    "that map the compressed representation back to the original input space.\n",
    "\n",
    "During training, the autoencoder is fed with input data, and the output of the decoder is \n",
    "compared with the original input data. The network's parameters (weights and biases) are \n",
    "adjusted to minimize the reconstruction error, typically measured using a loss function \n",
    "such as mean squared error (MSE) or binary cross-entropy.\n",
    "\n",
    "Once trained, the encoder network can be used to generate the compressed representation of \n",
    "new input data, while the decoder network can reconstruct the original data from this \n",
    "compressed representation.\n",
    "\n",
    "Vanilla autoencoders are versatile and can be applied to various tasks such as data denoising, \n",
    "feature learning, anomaly detection, and dimensionality reduction. However, they may struggle\n",
    "with capturing complex structures in the data and are often extended or modified to address \n",
    "specific challenges, such as adding regularization techniques, using different activation \n",
    "functions, or incorporating convolutional or recurrent layers for handling structured data\n",
    "like images or sequences.\"\"\"\n",
    "\n",
    "#2. What are Sparse autoencoders\n",
    "\n",
    "\"\"\"Sparse autoencoders are a variation of traditional autoencoder architectures designed to learn\n",
    "sparse representations of data. In a sparse autoencoder, the goal is not only to reconstruct the \n",
    "input data but also to encourage the learned representations to be sparse, meaning that only a\n",
    "small number of units in the hidden layers are active at a time.\n",
    "\n",
    "The sparsity constraint is imposed by adding a regularization term to the loss function during \n",
    "training. This regularization encourages the activation of only a subset of neurons in the hidden\n",
    "layers, leading to a more efficient representation of the input data.\n",
    "\n",
    "There are several ways to enforce sparsity in autoencoders:\n",
    "\n",
    "1. **L1 Regularization**: This method penalizes the absolute values of the weights in the network, \n",
    "promoting sparsity by driving many weights to zero.\n",
    "\n",
    "2. **Kullback-Leibler (KL) Divergence Regularization**: KL divergence is a measure of how one \n",
    "probability distribution differs from a second, expected probability distribution. In the context \n",
    "of sparse autoencoders, KL divergence regularization encourages the activation of hidden units to\n",
    "match a target sparsity level.\n",
    "\n",
    "3. **Dropout**: Dropout is a regularization technique commonly used in neural networks to prevent\n",
    "overfitting. In sparse autoencoders, dropout can be applied to the hidden layers to randomly\n",
    "deactivate a fraction of neurons during each training iteration, thereby encouraging the remaining\n",
    "active neurons to learn more meaningful representations.\n",
    "\n",
    "Sparse autoencoders are particularly useful in scenarios where interpretability and feature selection\n",
    "are important, as the sparse representations learned by the network can help identify the most relevant\n",
    "features in the input data. They have been applied successfully in various domains, including image and\n",
    "text data processing, where identifying key features or patterns is crucial.\"\"\"\n",
    "\n",
    "#3. What are Denoising autoencoders\n",
    "\n",
    "\"\"\"Denoising autoencoders are a type of autoencoder architecture designed to learn robust \n",
    "representations of data by reconstructing clean data from corrupted inputs. Unlike traditional\n",
    "autoencoders, which aim to reconstruct clean input data, denoising autoencoders are trained on \n",
    "noisy or corrupted versions of the input data.\n",
    "\n",
    "The key idea behind denoising autoencoders is to force the network to learn a representation of \n",
    "the underlying structure of the data that is resilient to noise or corruption. This can help in\n",
    "learning more meaningful and generalized features from the data, as the network needs to extract\n",
    "relevant information from noisy inputs.\n",
    "\n",
    "Here's how denoising autoencoders typically work:\n",
    "\n",
    "1. **Corruption Process**: During training, the input data is intentionally corrupted by adding \n",
    "noise or introducing some form of distortion. This corruption process can take various forms, \n",
    "such as adding Gaussian noise, masking random subsets of input values, or applying dropout to the input.\n",
    "\n",
    "2. **Reconstruction Objective**: The corrupted input data is fed into the autoencoder, which then\n",
    "attempts to reconstruct the original, clean input data. The reconstruction loss is computed by \n",
    "comparing the output of the decoder with the clean input data, encouraging the network to learn\n",
    "representations that capture the underlying structure of the data, rather than memorizing noise.\n",
    "\n",
    "3. **Training**: The network parameters (weights and biases) are adjusted during training using \n",
    "backpropagation and gradient descent to minimize the reconstruction error between the output of \n",
    "the decoder and the clean input data.\n",
    "\n",
    "Denoising autoencoders can learn robust representations of data that generalize well to unseen, \n",
    "clean data. They are useful for tasks such as data denoising, feature learning, and anomaly detection. \n",
    "By learning to reconstruct clean data from noisy inputs, denoising autoencoders can effectively filter \n",
    "out irrelevant or noisy information, leading to more robust and meaningful representations of the data.\"\"\"\n",
    "\n",
    "#4. What are Convolutional autoencoders\n",
    "\n",
    "\"\"\"Convolutional autoencoders are a type of autoencoder architecture that utilizes convolutional \n",
    "neural network (CNN) layers for both the encoder and decoder components. These autoencoders are\n",
    "particularly well-suited for tasks involving structured grid-like data, such as images, where\n",
    "spatial relationships between neighboring pixels are important.\n",
    "\n",
    "Here's how convolutional autoencoders work:\n",
    "\n",
    "1. **Encoder**: The encoder part of the network consists of one or more convolutional layers \n",
    "followed by pooling layers, which progressively reduce the spatial dimensions of the input \n",
    "data while increasing the number of feature maps (channels). This process helps extract \n",
    "hierarchical features from the input data.\n",
    "\n",
    "2. **Latent Space**: Similar to traditional autoencoders, convolutional autoencoders have a \n",
    "latent space where the compressed representation of the input data is stored. The convolutional \n",
    "layers in the encoder produce feature maps representing different aspects of the input data.\n",
    "\n",
    "3. **Decoder**: The decoder reverses the process of the encoder, using convolutional transpose\n",
    "layers (also known as deconvolutional layers or upsampling layers) to gradually upsample the \n",
    "feature maps while reducing the number of channels, ultimately producing an output that aims\n",
    "to reconstruct the original input data.\n",
    "\n",
    "4. **Training**: Convolutional autoencoders are trained using backpropagation and gradient descent, \n",
    "where the loss function typically measures the difference between the input data and the output of \n",
    "the decoder. The network learns to minimize this reconstruction error by adjusting its parameters \n",
    "(weights and biases) during training.\n",
    "\n",
    "Convolutional autoencoders are widely used in tasks such as image denoising, image inpainting \n",
    "(filling in missing parts of images), image compression, and feature learning from visual data.\n",
    "By leveraging convolutional layers, these autoencoders can capture spatial relationships and \n",
    "hierarchical features present in the input data, making them effective for tasks involving images\n",
    "or other structured data formats.\"\"\"\n",
    "\n",
    "#5. What are Stacked autoencoders\n",
    "\n",
    "\"\"\"Stacked autoencoders, also known as deep autoencoders or deep belief networks, are a type of\n",
    "autoencoder architecture composed of multiple layers of encoders and decoders. Each layer in a \n",
    "stacked autoencoder learns increasingly abstract representations of the input data.\n",
    "\n",
    "The architecture of a stacked autoencoder typically consists of an encoder-decoder pair for each\n",
    "layer, where the output of the encoder in one layer serves as the input to the decoder in the next \n",
    "layer. The first layer's encoder takes the raw input data, while the last layer's decoder produces \n",
    "the final reconstructed output. The intermediate layers are referred to as hidden layers.\n",
    "\n",
    "Here's how stacked autoencoders are trained:\n",
    "\n",
    "1. **Pre-training**: Each layer in the stacked autoencoder is pre-trained independently as a \n",
    "shallow autoencoder. During pre-training, the input data is fed into the first layer's encoder,\n",
    "and the reconstruction error is computed at the output of the first layer's decoder. This process\n",
    "is repeated for each subsequent layer, with the output of the previous layer serving as the input\n",
    "to the next layer. Pre-training is typically done using unsupervised learning techniques such as \n",
    "greedy layer-wise training or contrastive divergence.\n",
    "\n",
    "2. **Fine-tuning**: After pre-training, the entire stacked autoencoder is fine-tuned using supervised\n",
    "learning techniques. The network is trained end-to-end using backpropagation and gradient descent to\n",
    "minimize the reconstruction error between the input data and the final output of the stacked autoencoder.\n",
    "\n",
    "Stacked autoencoders are capable of learning complex hierarchical representations of the input data, \n",
    "making them well-suited for tasks such as feature learning, dimensionality reduction, and generative\n",
    "modeling. They can capture intricate patterns and relationships present in the data by leveraging \n",
    "multiple layers of abstraction. Stacked autoencoders have been successfully applied in various domains,\n",
    "including computer vision, natural language processing, and bioinformatics.\"\"\"\n",
    "\n",
    "#6. Explain how to generate sentences using LSTM autoencoders\n",
    "\n",
    "\"\"\"Generating sentences using LSTM (Long Short-Term Memory) autoencoders involves training a \n",
    "model to encode input sentences into a fixed-size latent representation and then decode these\n",
    "representations back into sentences. The training process involves feeding the model with a \n",
    "corpus of sentences, where it learns to reconstruct the input sentences accurately.\n",
    "\n",
    "Here's a step-by-step explanation of how to generate sentences using LSTM autoencoders:\n",
    "\n",
    "1. **Data Preparation**: Prepare a dataset of sentences for training the autoencoder. Tokenize \n",
    "the sentences into words or subwords, and convert them into numerical vectors (one-hot encoding \n",
    "or word embeddings).\n",
    "\n",
    "2. **Encoder Architecture**: Design the encoder part of the LSTM autoencoder. The encoder LSTM \n",
    "layer(s) will process the input sequences and produce a fixed-size latent representation. \n",
    "Optionally, you can stack multiple LSTM layers or use bidirectional LSTMs for more complex encoding.\n",
    "\n",
    "3. **Decoder Architecture**: Design the decoder part of the LSTM autoencoder. The decoder LSTM\n",
    "layer(s) will take the latent representation generated by the encoder and reconstruct the input\n",
    "sequence. The decoder output should match the input sequence dimensionality.\n",
    "\n",
    "4. **Model Training**: Train the LSTM autoencoder using the prepared dataset. The model learns to \n",
    "reconstruct the input sequences by minimizing a loss function (e.g., mean squared error or\n",
    "cross-entropy loss) between the input and reconstructed sequences.\n",
    "\n",
    "5. **Generating Sentences**: To generate new sentences using the trained autoencoder, you can \n",
    "follow these steps:\n",
    "   - Encode a seed sentence (or a latent vector sampled from a distribution) using the trained encoder.\n",
    "   - Optionally, modify the latent representation (e.g., adding noise or adjusting specific dimensions) \n",
    "   to generate diverse outputs.\n",
    "   - Decode the modified or sampled latent representation using the trained decoder.\n",
    "   - Repeat the decoding process until the end-of-sequence token is generated, or until a maximum \n",
    "   length is reached.\n",
    "\n",
    "6. **Decoding Strategy**: Depending on the task and the model's architecture, you can use different \n",
    "strategies for decoding, such as greedy decoding (selecting the token with the highest probability\n",
    "at each step), beam search (maintaining a list of candidate sequences), or sampling from the probability \n",
    "distribution over tokens (e.g., using softmax).\n",
    "\n",
    "7. **Evaluation**: Evaluate the generated sentences for quality and coherence using metrics such as \n",
    "perplexity, BLEU score, or human judgment.\n",
    "\n",
    "8. **Fine-tuning and Optimization**: Experiment with different hyperparameters, model architectures, \n",
    "and training strategies to improve the quality of generated sentences. Fine-tune the model on specific\n",
    "tasks or domains if needed.\n",
    "\n",
    "By training an LSTM autoencoder on a corpus of sentences and carefully designing the decoding process, \n",
    "you can generate coherent and meaningful sentences that capture the structure and semantics of the input data.\"\"\"\n",
    "\n",
    "#7. Explain Extractive summarization\n",
    "\n",
    "\"\"\"Extractive summarization is a technique used in natural language processing (NLP) to create a concise\n",
    "summary of a document by selecting and extracting important sentences or passages directly from the \n",
    "original text. Unlike abstractive summarization, which generates new sentences to convey the main \n",
    "ideas of the document, extractive summarization relies on identifying and retaining the most informative\n",
    "content already present in the document.\n",
    "\n",
    "Here's how extractive summarization typically works:\n",
    "\n",
    "1. **Text Preprocessing**: The input document is preprocessed to remove noise, such as formatting tags,\n",
    "punctuation, and stopwords (commonly occurring words with little semantic value).\n",
    "\n",
    "2. **Sentence Tokenization**: The document is segmented into individual sentences using a sentence\n",
    "tokenizer. Each sentence serves as a candidate for inclusion in the summary.\n",
    "\n",
    "3. **Feature Extraction**: Various features are computed for each sentence to assess its importance \n",
    "and relevance to the overall content of the document. Common features include:\n",
    "   - Word frequency: Sentences containing important keywords or phrases that appear frequently in the \n",
    "   document are considered more relevant.\n",
    "   - Sentence length: Longer sentences may contain more information, but excessively long sentences \n",
    "   might be less coherent or less focused.\n",
    "   - Position: Sentences appearing at the beginning or end of the document may be more likely to contain \n",
    "   important information.\n",
    "   - Named entities: Sentences containing named entities (such as people, organizations, or locations)\n",
    "   are often considered important.\n",
    "   - Semantic similarity: The similarity between sentences can help identify redundant or overlapping \n",
    "   information.\n",
    "\n",
    "4. **Scoring and Ranking**: Each sentence is assigned a score based on its computed features. \n",
    "The scoring mechanism may vary depending on the specific algorithm or model used for extractive\n",
    "summarization. Common approaches include:\n",
    "   - Weighted sum of feature values\n",
    "   - Machine learning models (e.g., Support Vector Machines, Random Forests) trained on labeled data\n",
    "   - Neural network models (e.g., Recurrent Neural Networks, Transformer-based models)\n",
    "\n",
    "5. **Sentence Selection**: The sentences with the highest scores are selected to form the summary. \n",
    "The number of sentences included in the summary can be predetermined or dynamically determined based \n",
    "on constraints such as a maximum summary length or a target compression ratio.\n",
    "\n",
    "6. **Summary Generation**: The selected sentences are concatenated to form the final summary.\n",
    "Optionally, post-processing steps such as sentence reordering or grammatical correction may be \n",
    "applied to improve the readability and coherence of the summary.\n",
    "\n",
    "Extractive summarization methods are relatively straightforward to implement and can produce summaries \n",
    "that faithfully represent the content of the original document. However, they may struggle with\n",
    "capturing the overall context, coherence, and abstraction present in the text, as they rely solely\n",
    "on selecting and rearranging existing content. Despite these limitations, extractive summarization\n",
    "techniques are widely used in applications where preserving the original wording and context is\n",
    "essential, such as news aggregation, document skimming, and content summarization for search engine\n",
    "snippets.\"\"\"\n",
    "\n",
    "#8. Explain Abstractive summarization\n",
    "\n",
    "\"\"\"Abstractive summarization is a technique used in natural language processing (NLP) to create \n",
    "a concise summary of a document by generating new sentences that convey the main ideas and\n",
    "information present in the original text. Unlike extractive summarization, which selects and \n",
    "rearranges existing sentences from the document, abstractive summarization involves understanding \n",
    "the content of the text and paraphrasing it in a more condensed and human-readable form.\n",
    "\n",
    "Here's how abstractive summarization typically works:\n",
    "\n",
    "1. **Text Preprocessing**: Similar to extractive summarization, the input document undergoes \n",
    "preprocessing steps such as removing noise, tokenization, and possibly lemmatization or stemming \n",
    "to normalize the text.\n",
    "\n",
    "2. **Content Understanding**: A model, often based on neural networks, processes the input\n",
    "document to understand its meaning and extract the key information. This may involve techniques \n",
    "such as attention mechanisms, which allow the model to focus on relevant parts of the text while \n",
    "generating the summary.\n",
    "\n",
    "3. **Summary Generation**: The model generates a summary by synthesizing new sentences that capture \n",
    "the essential information from the original document. This process often involves:\n",
    "   - Encoding the input document into a fixed-size representation using techniques such as recurrent\n",
    "   neural networks (RNNs), Long Short-Term Memory (LSTM) networks, or Transformer-based models like \n",
    "   BERT (Bidirectional Encoder Representations from Transformers).\n",
    "   - Decoding the encoded representation to generate a sequence of words that form the summary.\n",
    "   The decoding process may use autoregressive models such as recurrent or transformer decoders,\n",
    "   which generate one word at a time based on the previously generated words and the encoded \n",
    "   representation of the input document.\n",
    "\n",
    "4. **Language Generation**: The generated summary may undergo post-processing steps to improve\n",
    "its readability, coherence, and grammatical correctness. This can include tasks such as:\n",
    "   - Language modeling to ensure that the generated sentences are fluent and grammatically correct.\n",
    "   - Semantic coherence checks to ensure that the summary conveys the intended meaning of the original text.\n",
    "   - Length normalization to control the length of the summary and avoid verbosity.\n",
    "\n",
    "5. **Evaluation**: The generated summary is evaluated based on various metrics such as ROUGE \n",
    "(Recall-Oriented Understudy for Gisting Evaluation), BLEU (Bilingual Evaluation Understudy), \n",
    "or human judgment to assess its quality and effectiveness in capturing the main ideas of the \n",
    "original document.\n",
    "\n",
    "Abstractive summarization methods have the advantage of being able to generate concise and \n",
    "coherent summaries that go beyond simple sentence extraction. However, they also face challenges \n",
    "such as maintaining factual accuracy, preserving the intended meaning of the original text, and\n",
    "avoiding the generation of misleading or incorrect information. Despite these challenges, \n",
    "abstractive summarization techniques are increasingly used in applications such as document \n",
    "summarization, news summarization, and conversational agents where generating human-like\n",
    "responses is desired.\"\"\"\n",
    "\n",
    "#9. Explain Beam search\n",
    "\n",
    "\"\"\"Beam search is a search algorithm commonly used in natural language processing (NLP) tasks, \n",
    "such as machine translation, text generation, and speech recognition. It is particularly useful\n",
    "in scenarios where the search space is vast, such as generating sequences of words in abstractive\n",
    "summarization or machine translation tasks.\n",
    "\n",
    "Beam search is an extension of the greedy search algorithm, which selects the most likely \n",
    "candidate at each step based solely on the highest probability. In contrast, beam search\n",
    "maintains a fixed-size set of candidate sequences, known as the \"beam,\" at each step of \n",
    "the search. Instead of selecting only the most likely candidate, beam search explores\n",
    "multiple possible candidates simultaneously.\n",
    "\n",
    "Here's how beam search typically works:\n",
    "\n",
    "1. **Initialization**: At the beginning of the search, an initial sequence (usually the start\n",
    "symbol) is added to the beam as the only candidate.\n",
    "\n",
    "2. **Expansion**: At each step of the search, the beam is expanded by generating the next set\n",
    "of candidate sequences. For each candidate sequence in the current beam:\n",
    "   - The model predicts the probabilities of all possible next tokens (words or symbols) given\n",
    "   the current candidate sequence.\n",
    "   - The top-k candidates with the highest probabilities are selected to form the next beam,\n",
    "   where k is the beam width or beam size. These candidates become the new set of candidate \n",
    "   sequences for the next step.\n",
    "\n",
    "3. **Pruning**: Optionally, the beam may be pruned to remove low-probability candidates and\n",
    "reduce computational overhead. This can be done by selecting the top-k candidates with the \n",
    "highest accumulated probabilities, where the accumulation involves multiplying the probabilities\n",
    "of the individual tokens along the sequence.\n",
    "\n",
    "4. **Termination**: The search continues until a termination condition is met, such as reaching \n",
    "a maximum sequence length, encountering an end-of-sequence token, or exhausting the beam width.\n",
    "\n",
    "5. **Selection**: Once the search is complete, the final output sequence is selected from the \n",
    "candidates in the last beam. This can be done by choosing the sequence with the highest probability,\n",
    "or by applying additional criteria such as diversity or fluency.\n",
    "\n",
    "Beam search allows the model to explore multiple possible sequences in parallel, which can lead to\n",
    "more diverse and contextually coherent outputs compared to greedy search. However, beam search may\n",
    "suffer from issues such as generating repetitive or redundant sequences and getting stuck in local\n",
    "optima. Various techniques, such as length normalization, diverse beam search, or incorporating a\n",
    "length penalty, can be used to mitigate these issues and improve the effectiveness of beam search\n",
    "in NLP tasks.\"\"\"\n",
    "\n",
    "#10. Explain Length normalization\n",
    "\n",
    "\"\"\"Length normalization is a technique used to address the bias towards shorter sequences in \n",
    "beam search algorithms, particularly in tasks such as text generation or sequence generation,\n",
    "where the length of the output sequences can vary significantly. It is commonly employed to \n",
    "improve the diversity and quality of generated sequences by mitigating the tendency of beam \n",
    "search to favor shorter sequences over longer ones.\n",
    "\n",
    "In beam search, each candidate sequence is assigned a score based on its probability according \n",
    "to the model and possibly additional factors such as length penalty. Without length normalization, \n",
    "shorter sequences tend to have higher probabilities simply because they have fewer tokens and \n",
    "therefore fewer opportunities for the model to make mistakes.\n",
    "\n",
    "Length normalization adjusts the scores of candidate sequences to account for their lengths,\n",
    "ensuring that longer sequences are not penalized unfairly. There are several approaches to\n",
    "length normalization, but one common method involves dividing the score of each candidate \n",
    "sequence by a function of its length.\n",
    "\n",
    "One popular function used for length normalization is the length penalty, which penalizes \n",
    "longer sequences by scaling down their scores. The length penalty \\( LP \\) for a sequence \n",
    "of length \\( l \\) is calculated as:\n",
    "\n",
    "\\[ LP(l) = \\left( \\frac{{\\beta + l}}{{\\beta + 1}} \\right)^\\alpha \\]\n",
    "\n",
    "where:\n",
    "- \\( \\alpha \\) is a hyperparameter that controls the strength of the length penalty.\n",
    "- \\( \\beta \\) is a hyperparameter that determines the desired length bias. Higher values of\n",
    "\\( \\beta \\) encourage shorter sequences, while lower values encourage longer sequences.\n",
    "\n",
    "The length-normalized score \\( \\text{Score}_{\\text{norm}} \\) of a candidate sequence with score\n",
    "\\( \\text{Score} \\) and length \\( l \\) is computed as:\n",
    "\n",
    "\\[ \\text{Score}_{\\text{norm}} = \\frac{\\text{Score}}{LP(l)} \\]\n",
    "\n",
    "By dividing the original score by the length penalty, longer sequences are effectively scaled up,\n",
    "making them more competitive with shorter sequences during the selection process in beam search.\n",
    "\n",
    "Length normalization helps to encourage diversity in the generated sequences by preventing the\n",
    "model from favoring shorter outputs solely due to their lower probability of containing errors.\n",
    "It is a useful technique for improving the performance of beam search in tasks where generating\n",
    "sequences of varying lengths is desired, such as text summarization, machine translation, and\n",
    "dialogue generation. Adjusting the hyperparameters \\( \\alpha \\) and \\( \\beta \\) allows for \n",
    "fine-tuning the length normalization strategy based on the specific characteristics of the \n",
    "task and the dataset.\"\"\"\n",
    "\n",
    "#11. Explain Coverage normalization\n",
    "\n",
    "\"\"\"Coverage normalization is a technique used in sequence-to-sequence models, particularly\n",
    "in tasks like machine translation or text summarization, to address the issue of repeated\n",
    "or untranslated words in the generated output. It helps to ensure that the model attends \n",
    "to all parts of the input sequence during the generation process, thus improving the overall\n",
    "coherence and fluency of the generated text.\n",
    "\n",
    "In sequence-to-sequence models with attention mechanisms, such as the encoder-decoder \n",
    "architecture with attention or transformer models, attention scores are computed for \n",
    "each input token at each decoding step. These attention scores indicate the relevance \n",
    "of each input token to the generation of the current output token. Coverage normalization\n",
    "aims to encourage the model to distribute its attention more evenly across the input \n",
    "sequence over multiple decoding steps, reducing the likelihood of repeated or untranslated words.\n",
    "\n",
    "Here's how coverage normalization typically works:\n",
    "\n",
    "1. **Initialization**: At the beginning of decoding, a coverage vector is initialized to zeros. \n",
    "The coverage vector keeps track of the cumulative attention given to each input token across \n",
    "decoding steps.\n",
    "\n",
    "2. **Attention Calculation**: At each decoding step, the model computes attention scores over \n",
    "the input tokens using the coverage vector in addition to the usual context vectors obtained\n",
    "from the encoder outputs. The attention scores are combined with the coverage vector to produce \n",
    "modified attention scores that penalize tokens that have already received attention in previous\n",
    "decoding steps.\n",
    "\n",
    "3. **Coverage Update**: After computing the attention scores, the coverage vector is updated to \n",
    "reflect the attention given to each input token at the current decoding step. The coverage vector\n",
    "is incremented by the attention scores obtained at the current step.\n",
    "\n",
    "4. **Coverage Penalty**: The coverage vector is used to penalize tokens that have already been \n",
    "attended to in previous decoding steps. This penalty is applied during the computation of the \n",
    "attention scores, discouraging the model from repeatedly attending to the same tokens.\n",
    "\n",
    "5. **Attention Combination**: The modified attention scores, which incorporate the coverage penalty,\n",
    "are used to compute the context vector for generating the current output token. This context vector\n",
    "is then combined with the decoder's hidden state and input embeddings to predict the next token in \n",
    "the output sequence.\n",
    "\n",
    "By incorporating coverage normalization into the attention mechanism, the model is encouraged to\n",
    "distribute its attention more evenly across the input sequence, leading to more fluent and coherent \n",
    "translations or summaries. Coverage normalization helps to mitigate issues such as repetition, \n",
    "omission, and inconsistency in the generated text, resulting in higher-quality outputs.\"\"\"\n",
    "\n",
    "#12. Explain ROUGE metric evaluation\n",
    "\n",
    "\"\"\"ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics commonly\n",
    "used for evaluating the quality of automatic summarization or machine translation systems \n",
    "by comparing their generated summaries or translations to reference summaries or translations \n",
    "created by humans. ROUGE measures the overlap between the generated and reference texts in \n",
    "terms of n-gram overlap, word overlap, and other similarity measures.\n",
    "\n",
    "There are several variants of the ROUGE metric, each focusing on a different aspect of text similarity:\n",
    "\n",
    "1. **ROUGE-N**: ROUGE-N measures the overlap of n-grams (contiguous sequences of n words) \n",
    "between the generated and reference texts. It computes precision, recall, and F1-score based \n",
    "on the count of overlapping n-grams. ROUGE-1 measures unigram overlap, ROUGE-2 measures bigram\n",
    "overlap, and so on.\n",
    "\n",
    "2. **ROUGE-L**: ROUGE-L measures the longest common subsequence (LCS) between the generated and\n",
    "reference texts. It calculates precision, recall, and F1-score based on the length of the LCS\n",
    "normalized by the lengths of the generated and reference texts. ROUGE-L is particularly useful \n",
    "for evaluating the fluency and coherence of generated texts.\n",
    "\n",
    "3. **ROUGE-W**: ROUGE-W measures the weighted LCS between the generated and reference texts, \n",
    "where the weight of each matching word is inversely proportional to its distance from the\n",
    "previous matching word. This variant of ROUGE-L gives more weight to consecutive matches, \n",
    "penalizing non-contiguous matches.\n",
    "\n",
    "4. **ROUGE-S**: ROUGE-S measures skip-bigram overlap between the generated and reference texts. \n",
    "Skip-bigrams are pairs of words that appear in the same order with at most k intervening words \n",
    "between them. ROUGE-S computes precision, recall, and F1-score based on the count of overlapping \n",
    "skip-bigrams.\n",
    "\n",
    "5. **ROUGE-SU**: ROUGE-SU is an extension of ROUGE-S that considers skip-bigrams of varying lengths\n",
    "(unigrams, bigrams, trigrams, etc.). It measures the overlap of these skip-bigrams between the \n",
    "generated and reference texts.\n",
    "\n",
    "ROUGE metrics are typically reported as precision, recall, and F1-score, which provide a comprehensive\n",
    "evaluation of the similarity between the generated and reference texts. A higher precision indicates\n",
    "that the generated text contains fewer extraneous elements, while a higher recall indicates that the\n",
    "generated text captures more of the reference text's content. The F1-score balances precision and recall, \n",
    "providing a single metric for overall performance evaluation.\n",
    "\n",
    "ROUGE metrics are widely used in research and development of automatic summarization and machine \n",
    "translation systems, providing objective measures of system performance that correlate well with\n",
    "human judgments of summary or translation quality.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
